{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJDYs1Mfl32j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Prereqs\n",
        "\n",
        "- OS: macOS, Windows, or Linux\n",
        "\n",
        "- Python: 3.9+\n",
        "\n",
        "- Hardware: 8‚Äì16 GB RAM recommended. A GPU helps, but CPU is fine for small models."
      ],
      "metadata": {
        "id": "OiZdPFlUCArH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Install Ollama"
      ],
      "metadata": {
        "id": "JxsnGuGsCRB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brew install ollama"
      ],
      "metadata": {
        "id": "NEFsTvOKIi4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2) Start the server\n",
        "\n",
        "Usually auto-starts. If not use the following command"
      ],
      "metadata": {
        "id": "nhkmyPz3JOjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama serve"
      ],
      "metadata": {
        "id": "M3J1lFUhJj1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above command will start the server, and we want to keep the server start, so open a new terminal window for next steps.\n",
        "Keep ollama serve running in one terminal window and do your work in another."
      ],
      "metadata": {
        "id": "dzsR8Ux82l6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3) Pull a chat model<br>\n",
        "Pull command, downloads the Modelfile which is the file (or set of files) that contains all the information for a trained AI model to run.<br>\n",
        "It contains:\n",
        "- Weights / Parameters: Numbers that the model learned during training (the ‚Äúbrain‚Äù of the AI).\n",
        "\n",
        "- Configuration / Architecture: How the model is built (layers, attention heads, hidden units, etc.).\n",
        "\n",
        "- Vocabulary / Tokenizer info: Helps the model understand words, sentences, or tokens.\n"
      ],
      "metadata": {
        "id": "8PTBLNXbJkhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama pull llama3:8b"
      ],
      "metadata": {
        "id": "lb9HaZGmJw2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Popular Chat Models You Could Use\n",
        "1. OpenAI GPT (e.g., GPT-4, GPT-3.5)\n",
        "\n",
        "- Pros: Best accuracy, handles complex queries, huge ecosystem (ChatGPT, API).\n",
        "\n",
        "- Cons: Paid (metered by tokens), requires internet (cloud-based).\n",
        "\n",
        "2. Anthropic Claude (Claude 3)\n",
        "\n",
        "- Pros: Strong reasoning, long context windows, safer outputs.\n",
        "\n",
        "- Cons: Paid, cloud-based only, no local running.\n",
        "\n",
        "3. Google Gemini (Gemini 1.5)\n",
        "\n",
        "- Pros: Good with multimodal (text + images), very large context sizes.\n",
        "\n",
        "- Cons: Paid, cloud-based, newer ecosystem.\n",
        "\n",
        "4. Mistral / Mixtral\n",
        "\n",
        "- Pros: Open-source, lightweight, can run locally (good for experiments), gives speed and efficiency on modest hardware.\n",
        "\n",
        "- Cons: Smaller than GPT-4, less consistent on complex reasoning.\n",
        "\n",
        "5. LLaMA (Large Language Model Accelerator, Developed by Meta AI)\n",
        "\n",
        "- Pros: Open-source, widely used, multiple sizes (7B, 13B, 70B) depending on your machine, you need high accuracy and you can afford more compute.\n",
        "\n",
        "- Cons: Needs proper serving (Ollama, LM Studio, Hugging Face, etc.).\n",
        "\n",
        "6. Ollama (Framework that handles running the model for us)\n",
        "\n",
        "- Lets us run open-source models locally.\n",
        "- Handles GPU/CPU optimization, easy API (localhost:11434).\n",
        "- Perfect for quick prototyping without paying API fees.\n",
        "- Easy Setup ‚Äî just ollama serve and the API is ready.\n",
        "- Privacy ‚Äî IT queries stay on your laptop, not sent to cloud.\n",
        "- Supports Many Models ‚Äî you can swap llama2, mistral, gemma, etc.\n",
        "- Great for our Requirement ‚Äî since we wanted to test on 5‚Äì10 IT support scenarios, a lightweight local model is more than enough. LLaMA 3 model (from Meta).\n",
        "- Our version is 8B (8 billion parameters ‚Üí a ‚Äúsmall/medium‚Äù size that runs on many modern laptops)."
      ],
      "metadata": {
        "id": "R4ajXMBW3O5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4) Quick terminal test\n",
        "We can omit this test because our goal is to use the model through a program or browser, like the upcoming Python chatbot. <br>\n",
        "Use ctrl+d to exit the chatbot."
      ],
      "metadata": {
        "id": "momcwR7-KHKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama run llama3:8b"
      ],
      "metadata": {
        "id": "uLslsJcZKPUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Create a Helpdesk ‚ÄúPersona‚Äù (Modelfile)\n",
        "Save this code as Helpdesk.Modelfile <br>\n",
        "With 'pull' we downloaded the base model from Ollama.\n",
        "\n",
        "It contains the pretrained AI brain ‚Äî it can answer general questions, do reasoning, etc.\n",
        "\n",
        "But it doesn‚Äôt know our specific IT helpdesk context (like company policies, email systems, or common IT issues). So it isn't enough?\n",
        "\n",
        "In other words its fine-tuning / specialization. Creating our custom version of LLaMA, now specialized for IT support."
      ],
      "metadata": {
        "id": "deCSkvkJKQhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1) Modelfile"
      ],
      "metadata": {
        "id": "BehGZnuJKyXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FROM llama3:8b \\\\start from base model\n",
        "SYSTEM \"\"\"\n",
        "You are an IT Helpdesk assistant. Write clear, step-by-step instructions.\n",
        "Prioritize safety and least-privilege actions. Ask 1 clarifying question if needed,\n",
        "but provide an initial checklist first. Keep answers short and actionable.\n",
        "If the issue is risky or needs admin rights, warn the user.\n",
        "\"\"\"\n",
        "#this is System Prompt which stays fixed, role based and are kind of rules. Actual questions by users are called User Prompt which changes everytime. Both keep the chatbot in its role.\n",
        "PARAMETER temperature 0.3\n",
        "#PARAMETER temperature 0.3 controls how creative vs. focused the chatbot is.Low temperature (0.1 ‚Äì 0.3) ‚Üí The bot is serious, consistent, and less random. Good for IT Helpdesk (clear, step-by-step answers). High temperature (0.7 ‚Äì 1.0) ‚Üí The bot is more creative, varied, and chatty. Good for marketing, brainstorming, or writing stories. Temperature = creativity knob. Lower = predictable, to-the-point. Higher = more variety, less predictable.\n",
        "PARAMETER num_ctx 4096\n",
        "#num_ctx 4096 = the model‚Äôs working memory, about 3,000 words. It lets our chatbot remember the current conversation and instructions without losing track. Bigger context = model can handle longer conversations or bigger documents. If the chat goes beyond this limit, the oldest messages get dropped and the model may ‚Äúforget‚Äù them. Smaller context = less memory, but uses less RAM/VRAM and runs faster. If you had num_ctx 8192 or 16384, you could feed in manuals, FAQs, or logs for even richer answers.\"Restart\" ‚Üí 1 token. \"the\" ‚Üí 1 token. \"com\" + \"puter\" ‚Üí 2 tokens. \".\" ‚Üí 1 token"
      ],
      "metadata": {
        "id": "kBtQtT1mKfRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2) Create the model"
      ],
      "metadata": {
        "id": "4LXDuTl9LDH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama create helpdesk -f Helpdesk.Modelfile\n",
        "# This command creates a new Ollama model called helpdesk using the instructions and configuration defined in Helpdesk.Modelfile. After this, you can run this helpdesk model locally to answer IT support questions or simulate a helpdesk agent."
      ],
      "metadata": {
        "id": "5oDSmJpQLFs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3) Sanity check"
      ],
      "metadata": {
        "id": "2HZeBNVPLJR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ollama run helpdesk\n",
        "# with this command we are confirming the model is sane enough to run before moving on to more detailed testing or deployment. With this command we can see the chatbot open and ready in our terminal window.\n",
        "# But we dont want to run it in th terminal. So we create basic chat client in next step."
      ],
      "metadata": {
        "id": "C0Ms_OaxLNsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Basic Python Chat Client (through local REST API)\n",
        "This calls http://localhost:11434/api/chat and streams the reply.<br>\n",
        "Ollama can turn your model into a service on your own computer that you can talk to using normal web requests (like how websites work). Think of it like setting up a mini website for your AI model, but it‚Äôs private and only you can use it on your computer.<br>\n",
        "Save this file as chat_client.py"
      ],
      "metadata": {
        "id": "zMg81gUVs2Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: chat_client.py\n",
        "\n",
        "# 1.We need to import requests because:it lets Python to talk  with websites or web-based APIs like Ollama. It simplifies sending and receiving HTTP requests. It handles JSON, headers, status codes etc.\n",
        "import requests\n",
        "\n",
        "# 2.The function defined here will chat with ollama\n",
        "def chat_with_ollama(messages, model=\"helpdesk\", stream=True):\n",
        "  #message- what we want to say to the AI\n",
        "  #model- which AI 'brain' to use (in our case helpdesk)\n",
        "  #stream- true=live, false=all at once\n",
        "\n",
        "# 3.Set server address and data\n",
        "    url = \"http://localhost:11434/api/chat\" #at this url, the AI is linstening on our computer\n",
        "    payload = { #payload which is a python dictionary is like a box which contains everything the AI needs, like model, message and stream\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": stream\n",
        "    }\n",
        "\n",
        "# 4.Sending the request ie our box of information to the AI\n",
        "    with requests.post(url, json=payload, stream=stream) as r: #Sending the request\n",
        "        r.raise_for_status() #checks for erros\n",
        "\n",
        "# 5. Read live streaming response, covering both cases: stream 'on' (small peices, like real conversation) or 'off' (whole reply)\n",
        "        if stream:\n",
        "            full = \"\"\n",
        "            for line in r.iter_lines():\n",
        "                if not line:\n",
        "                    continue\n",
        "                part = line.decode(\"utf-8\")\n",
        "                # Each line is a JSON chunk like: {\"message\":{\"role\":\"assistant\",\"content\":\"...\"},\"done\":false}\n",
        "                # We'll extract content safely:\n",
        "                try:\n",
        "                    import json\n",
        "                    obj = json.loads(part)\n",
        "                    chunk = obj.get(\"message\", {}).get(\"content\", \"\")\n",
        "                    print(chunk, end=\"\", flush=True)\n",
        "                    full += chunk\n",
        "                    if obj.get(\"done\"):\n",
        "                        break\n",
        "                except Exception:\n",
        "                    pass\n",
        "            print()\n",
        "            return full\n",
        "\n",
        "# 6.Non-streaming response\n",
        "        else:\n",
        "            data = r.json()\n",
        "            content = data[\"message\"][\"content\"]\n",
        "            print(content)\n",
        "            return content\n",
        "\n",
        "# 7.This is the Main block. system message tells AI how to behave and user message is what the user wants help with. AI replied according to both of these\n",
        "if __name__ == \"__main__\":\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": \"Follow the helpdesk rules.\"},\n",
        "        {\"role\": \"user\", \"content\": \"My email isn't sending‚Äîhelp!\"}\n",
        "    ]\n",
        "    chat_with_ollama(msgs)\n"
      ],
      "metadata": {
        "id": "vgHx55H3tCbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1) Simple Web UI (Using Streamlit)\n",
        "Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science projects. <br>\n",
        "Save this file as app.py"
      ],
      "metadata": {
        "id": "IU698OkbtE4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file: app.\n",
        "# 1. Imports\n",
        "import streamlit as st #streamlit makes web app UI\n",
        "import requests # requests sends messages to your local Ollama REST API.\n",
        "import json #json handles the JSON data coming back from the model.\n",
        "\n",
        "# 2. Page Setup\n",
        "st.set_page_config(page_title=\"IT Helpdesk Chatbot for IT 7133\", page_icon=\"üí¨\") # Sets the web app‚Äôs title and emoji icon.\n",
        "st.title(\"üí¨ IT Helpdesk (Ollama)\") # Sets the web app‚Äôs title and emoji icon.\n",
        "\n",
        "# 3. Sidebar Controls\n",
        "model = st.sidebar.text_input(\"Model\", value=\"helpdesk\") #Lets you choose which model to use (default: helpdesk).\n",
        "temperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.3, 0.1) #temperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.3, 0.1)\n",
        "\n",
        "# 4. Chat history setup\n",
        "# Keeps track of the whole conversation (system, user, assistant).\n",
        "# Starts with a system role = ‚ÄúYou are an IT Helpdesk assistant.‚Äù\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state.history = [{\"role\":\"system\",\"content\":\"You are an IT Helpdesk assistant.\"}]\n",
        "\n",
        "# 5. Display past messages\n",
        "# Loops through old messages and shows them in the chat UI.\n",
        "for m in st.session_state.history:\n",
        "    if m[\"role\"] != \"system\":\n",
        "        with st.chat_message(m[\"role\"]):\n",
        "            st.markdown(m[\"content\"])\n",
        "\n",
        "# 6. Handle new user input\n",
        "# lets user type a new message and add it to the conversation history\n",
        "user_msg = st.chat_input(\"Describe your IT issue...\")\n",
        "if user_msg:\n",
        "    st.session_state.history.append({\"role\":\"user\",\"content\":user_msg})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_msg)\n",
        "\n",
        "# 7. Send request to Ollama\n",
        "# Sends the full chat history to the local Ollama API.\n",
        "# Includes chosen model + temperature.\n",
        "# Uses streaming, so responses arrive chunk by chunk.\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        url = \"http://localhost:11434/api/chat\"\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": st.session_state.history,\n",
        "            \"options\": {\"temperature\": temperature},\n",
        "            \"stream\": True\n",
        "        }\n",
        "        resp = requests.post(url, json=payload, stream=True)\n",
        "\n",
        "# 8. Stream and display model‚Äôs reply\n",
        "# As the model replies, each chunk is displayed in real-time.\n",
        "# full collects the entire response.\n",
        "\n",
        "        full = \"\"\n",
        "        spot = st.empty()\n",
        "        for line in resp.iter_lines():\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line.decode(\"utf-8\"))\n",
        "                chunk = obj.get(\"message\", {}).get(\"content\", \"\")\n",
        "                full += chunk\n",
        "                spot.markdown(full)\n",
        "                if obj.get(\"done\"):\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "# 9. Save assistant‚Äôs reply to history\n",
        "# Adds the model‚Äôs final reply to the conversation history.\n",
        "# Ensures the chatbot ‚Äúremembers‚Äù for the next turn.\n",
        "        st.session_state.history.append({\"role\":\"assistant\",\"content\":full})\n",
        "\n",
        "# In short, This code makes a web-based IT Helpdesk chatbot:\n",
        "# You open it in your browser.\n",
        "# Type your IT issue.\n",
        "# The message goes to your local Ollama model (helpdesk) via REST API.\n",
        "# The model streams its reply back in real-time.\n",
        "# Streamlit shows the conversation history like a chat app."
      ],
      "metadata": {
        "id": "UQK99m8BtRua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2)Run the file"
      ],
      "metadata": {
        "id": "7uMoSEB_tWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit requests\n",
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "3-Tc_inctgsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file will run and the chatbot will open up in the browser. <br>\n",
        "Closing the browser tab doesn‚Äôt stop it, the server is still alive in the background. ctrl+c is used to stop the server."
      ],
      "metadata": {
        "id": "O2p6hy2Rtq3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Safety Tips\n",
        "\n",
        "- Temperature: start at 0.2‚Äì0.4 for consistent, checklist-style answers.\n",
        "\n",
        "- Refuse risky steps: bake warnings into the System prompt (already done).\n",
        "\n",
        "- Timebox outputs: ‚ÄúKeep it under 10 steps unless critical.‚Äù\n",
        "\n",
        "- Ask 1 clarifying question only (prevents endless loops)."
      ],
      "metadata": {
        "id": "mf5b4uYSuAb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make It Feel ‚ÄúIT-Desk Real‚Äù\n",
        "\n",
        "**Triage first: network? account? device? scope?** <br>\n",
        "Triage = figure out what type of problem the user has before giving solutions.<br>\n",
        "Ask yourself:\n",
        "- Is it a network problem? (Wi-Fi, VPN)\n",
        "- Is it an account problem? (login, password)\n",
        "- Is it a device problem? (laptop, phone, printer)\n",
        "- What is the scope? (affects one user, a department, or the whole company)\n",
        "- This ensures the chatbot responds appropriately instead of giving generic answers.\n",
        "\n",
        "**Least privilege: user steps first; admin steps only if necessary**\n",
        "\n",
        "- Always start with actions a normal user can do.\n",
        "- Only suggest admin-level actions (like changing system settings or server configs) if absolutely needed.\n",
        "- Helps prevent mistakes and keeps advice safe and practical.\n",
        "\n",
        "**Escalation hooks: ‚ÄúIf step X fails, open a ticket with code Y and attach logs Z.‚Äù**\n",
        "\n",
        "- Real IT helpdesk has escalation procedures.\n",
        "The chatbot should guide the user:\n",
        "- If a fix fails ‚Üí don‚Äôt panic; follow a predefined escalation process.\n",
        "- Include ticket codes, logs, screenshots for the next support tier.\n",
        "Makes the bot behave like a real IT team member.\n",
        "\n",
        "**Environment variables: point the bot to your help articles later (see RAG below)**\n",
        "\n",
        "- You can give the chatbot links or references to your internal knowledge base (like help articles).\n",
        "- Environment variables or configuration settings let the bot dynamically fetch relevant info instead of hardcoding everything.\n",
        "- RAG (Retrieval-Augmented Generation): a method where the model retrieves relevant documents and uses them to answer questions more accurately."
      ],
      "metadata": {
        "id": "OUWhfDwCuL1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nice Upgrades\n",
        "\n",
        "- RAG (knowledge base): add your FAQs/KB PDFs and do retrieval ‚Üí pass summaries into the prompt.\n",
        "\n",
        "- Ticketing integration: push a summary to Jira/ServiceNow via webhooks.\n",
        "\n",
        "- Logging: store conversations + scores from eval.py to a CSV for tracking improvements.\n",
        "\n",
        "- Custom models: create multiple Modelfiles (e.g., helpdesk_windows, helpdesk_mac)."
      ],
      "metadata": {
        "id": "Ex32Q5s1uTs8"
      }
    }
  ]
}